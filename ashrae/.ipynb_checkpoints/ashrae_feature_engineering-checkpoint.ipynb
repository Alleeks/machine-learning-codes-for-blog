{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install meteocalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なモジュールのimport\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 150) # pandas dataframe表示列数の設定\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import seaborn as sns\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import datetime\n",
    "\n",
    "from meteocalc import feels_like, Temp\n",
    "import os, random, math, psutil, pickle\n",
    "\n",
    "# kaggle kernelの利用を想定しています。\n",
    "# ローカルなど、別環境で作業する場合はファイルを置いたディレクトリを指定してください\n",
    "print(os.listdir('../input/ashrae-energy-prediction/')) \n",
    "\n",
    "# 今回はトレーニングデータのみで進めていきます\n",
    "train_df = pd.read_csv('../input/ashrae-energy-prediction/train.csv')\n",
    "weather_train_df = pd.read_csv('../input/ashrae-energy-prediction/weather_train.csv')\n",
    "building_meta_df = pd.read_csv('../input/ashrae-energy-prediction/building_metadata.csv')\n",
    "\n",
    "# train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "# weather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['meter_reading'] = np.log1p(train_df['meter_reading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_weather_dataset(weather_df):\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df[\"timestamp\"].min(), time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df[\"timestamp\"].max(), time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600)/3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df[\"site_id\"]==site_id][\"timestamp\"])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list, site_hours), columns=[\"timestamp\"])\n",
    "        new_rows[\"site_id\"] = site_id\n",
    "        weather_df = pd.concat([weather_df, new_rows])\n",
    "        \n",
    "        weather_df = weather_df.reset_index(drop=True)\n",
    "        \n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    weather_df = weather_df.set_index([\"site_id\", \"day\", \"month\"])\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby([\"site_id\", \"day\", \"month\"])[\"air_temperature\"].mean(), columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler, overwrite=False)\n",
    "    \n",
    "    cloud_coverage_filler = weather_df.groupby([\"site_id\", \"day\", \"month\"])[\"cloud_coverage\"].mean()\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method=\"ffill\"), columns=[\"cloud_coverage\"])\n",
    "    weather_df.update(cloud_coverage_filler, overwrite=False)\n",
    "    \n",
    "    dew_temperature_filler = pd.DataFrame(weather_df.groupby([\"site_id\", \"day\", \"month\"])[\"dew_temperature\"].mean(), columns=[\"dew_temperature\"])\n",
    "    weather_df.update(dew_temperature_filler, overwrite=False)\n",
    "    \n",
    "    sea_level_filler = weather_df.groupby([\"site_id\", \"day\", \"month\"])[\"sea_level_pressure\"].mean()\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method=\"ffill\"), columns=[\"sea_level_pressure\"])\n",
    "    \n",
    "    weather_df.update(sea_level_filler, overwrite=False)\n",
    "    \n",
    "    wind_direction_filler = pd.DataFrame(weather_df.groupby([\"site_id\", \"day\", \"month\"])[\"wind_direction\"].mean(), columns=[\"wind_direction\"])\n",
    "    weather_df.update(wind_direction_filler, overwrite=False)\n",
    "    \n",
    "    wind_speed_filler = pd.DataFrame(weather_df.groupby([\"site_id\", \"day\", \"month\"])[\"wind_speed\"].mean(), columns=[\"wind_speed\"])\n",
    "    weather_df.update(wind_speed_filler, overwrite=False)\n",
    "    \n",
    "    precip_depth_filler = weather_df.groupby([\"site_id\", \"day\", \"month\"])[\"precip_depth_1_hr\"].mean()\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method=\"ffill\"), columns=[\"precip_depth_1_hr\"])\n",
    "    weather_df.update(precip_depth_filler, overwrite=False)\n",
    "    \n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop([\"datetime\", \"day\", \"week\", \"month\"], axis=1)\n",
    "    \n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使い方は以下の通りです\n",
    "weather_train_df = fill_weather_dataset(weather_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meteorological_features(data):\n",
    "        def calculate_rh(df):\n",
    "            df['relative_humidity'] = 100 * (np.exp((17.625 * df['dew_temperature']) / (243.04 + df['dew_temperature'])) / np.exp((17.625 * df['air_temperature'])/(243.04 + df['air_temperature'])))\n",
    "        def calculate_fl(df):\n",
    "            flike_final = []\n",
    "            flike = []\n",
    "            # calculate Feels Like temperature\n",
    "            for i in range(len(df)):\n",
    "                at = df['air_temperature'][i]\n",
    "                rh = df['relative_humidity'][i]\n",
    "                ws = df['wind_speed'][i]\n",
    "                flike.append(feels_like(Temp(at, unit = 'C'), rh, ws))\n",
    "            for i in range(len(flike)):\n",
    "                flike_final.append(flike[i].f)\n",
    "            df['feels_like'] = flike_final\n",
    "            del flike_final, flike, at, rh, ws\n",
    "        calculate_rh(data)\n",
    "        calculate_fl(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下のように関数を呼び出すことで、特徴量が生成されます（最終2列が生成した特徴量）\n",
    "weather_train_df = get_meteorological_features(weather_train_df)\n",
    "weather_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"building_id\"と\"meter\"の値をstr型で結合して、\"building_meter\"カラムとします\n",
    "# 例: building_id=1111, meter=0→building_meter=1111_0\n",
    "train_df['building_meter'] = list(map(lambda x, y: str(x) + '_' + str(y), train_df['building_id'], train_df['meter']))\n",
    "\n",
    "# 作成した\"building_meter\"カラムの値をuniqueで重複なく取り出しリスト化\n",
    "all_building_meter = list(train_df['building_meter'].unique())\n",
    "# \"building_meter\"の値と対応する数値を結びつける辞書を作成\n",
    "# 例: 1111_0 → 50(値はてきとうです)\n",
    "building_meter_map = dict(zip(all_building_meter, np.arange(len(all_building_meter))))\n",
    "\n",
    "# 作成した辞書を用いて、building_meterの値をマッピングする処理をします\n",
    "train_df['building_meter_category'] = train_df['building_meter'].map(building_meter_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'building_meter'カラムには'building_id'と'meter'を文字列結合したものが格納され、\n",
    "# その値と一対一に対応する数値が'building_meter_category'に格納されています\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "building_meta_df[\"primary_use\"] = le.fit_transform(building_meta_df[\"primary_use\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先にテーブルを結合しておきます\n",
    "train_df = train_df.merge(building_meta_df, left_on=\"building_id\", right_on=\"building_id\", how=\"left\")\n",
    "train_df = train_df.merge(weather_train_df, how=\"left\", left_on=[\"site_id\", \"timestamp\"], right_on=[\"site_id\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングデータの\"meter_reading\"の値を\"target\"変数で保持\n",
    "train_df[\"meter_reading\"] = np.log1p(train_df[\"meter_reading\"])\n",
    "\n",
    "# また、学習に用いるデータは、目的変数の\"meter_reading\"をはじめ、使用しないカラム名を指定して、削除します\n",
    "train_df = train_df.drop([\"timestamp\", \"year_built\", \"floor_count\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\", \"building_meter\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
